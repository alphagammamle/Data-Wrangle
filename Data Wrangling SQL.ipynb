{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenStreetMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenStreetMap (OSM) foundation is building free and editable map of the world, enabling the development of freely-reusable geospatial data. The data from OpenStreetMap is being used by many applications such as GoogleMaps, Foursquare and Craigslist. \n",
    "\n",
    "To look at the map, or download your area of interest, you can visit http://www.openstreetmap.org website. \n",
    "\n",
    "For more information you can check their wiki which includes all the necessary information and documentation:\n",
    "https://en.wikipedia.org/wiki/OpenStreetMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area Chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I chose Chicago, the Windy City in the US. This is the city where I got my undergrad degree.\n",
    "\n",
    "The original file is about 2 GB in size; I use a sample file about 50MB to perform my initial analysis on. Finally, I run it on the original file to create the CSV files for my database. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ET.iterparse (i.e. iterative parsing) is efficient here since the original file is too large for processing the whole thing.\n",
    "\n",
    "The main problem we encountered in the dataset is the street name inconsistencies. Below is the old name corrected with the better name. \n",
    "- Avenue (starting with capital letter)\n",
    "- Ave\n",
    "- Ave.\n",
    "- avenue (starting with small letter)\n",
    "\n",
    "To be able to process the data, we need to make these street types uniform. In case we are later searching for specific Avenue names, we can do a quick search on all street types that have the word 'Avenue' in them and we can make sure that we are not missing anything with abrreviations of Avenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 2586,\n",
      " 'nd': 53000,\n",
      " 'node': 43731,\n",
      " 'osm': 1,\n",
      " 'relation': 48,\n",
      " 'tag': 16952,\n",
      " 'way': 6097}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "\n",
    "OSMFILE = 'map.osm'\n",
    "\n",
    "def count_tags(filename):\n",
    "    tags= {}\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if elem.tag not in tags.keys():\n",
    "            tags[elem.tag] = 1\n",
    "        else:\n",
    "            tags[elem.tag] += 1\n",
    "    \n",
    "    pprint.pprint(tags)\n",
    "    \n",
    "count_tags(OSMFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 11580, 'lower_colon': 5117, 'other': 255, 'problemchars': 0}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "OSMFILE = 'map.osm'\n",
    "\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        for tag in element.iter('tag'): #iterating through the tag element in the XML file\n",
    "            k = element.attrib['k'] #looking for the tag attribute 'k' which contains the keys\n",
    "            if re.search(lower, k):\n",
    "                keys['lower'] += 1\n",
    "            elif re.search(lower_colon, k):\n",
    "                keys['lower_colon'] += 1\n",
    "            elif re.search(problemchars, k):\n",
    "                keys['problemchars'] += 1\n",
    "            else:\n",
    "                keys['other'] += 1\n",
    "                \n",
    "    return keys\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    pprint.pprint(keys)\n",
    "    \n",
    "process_map(OSMFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146\n"
     ]
    }
   ],
   "source": [
    "OSMFILE = 'map.osm'\n",
    "\n",
    "def process_map(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if element.tag == 'node' or element.tag == 'way' or element.tag == 'relation':\n",
    "                userid = element.attrib['uid']\n",
    "                users.add(userid)\n",
    "\n",
    "    print len(users)\n",
    "    \n",
    "process_map(OSMFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows the users that contribute to the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "OSMFILE = 'map.osm'\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "# the list of street types that we want to have\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'audit_street_type' function will get the list of street types and using the regular expression, compare them to the expected list. If they do not match the names in the expected list, it adds it to the street_types dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The 'is_street_name' function will get the elements in the file\n",
    "(i.e. the tag element) and return the attributes in that element for which their \n",
    "key is equal to 'addr:street'. \n",
    "The 'audit' funntion uses iterative parsing to go through the XML file,\n",
    "parse node and way elements, and iterate through their tag element. \n",
    "It will then call the 'audit_street_type' function to add the value attribute \n",
    "of the tag (i.e. the street name) to it. \n",
    "'''\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    \n",
    "    #parses the XML file\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        # iterate through the 'tag' element of node and way elements\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'52': set(['US 52']),\n",
      " 'Ave.': set(['W. Stadium Ave.']),\n",
      " 'D': set(['Cumberland Ave #D']),\n",
      " 'Dr': set(['Agriculture Mall Dr']),\n",
      " 'Lagrange': set(['Hamilton & Lagrange']),\n",
      " 'Mall': set(['Memorial Mall', 'Purdue Mall', 'Stadium Mall']),\n",
      " 'North': set(['West 350 North']),\n",
      " 'Rd': set(['Klondike Rd']),\n",
      " 'St': set(['East State St', 'W Wood St']),\n",
      " 'St.': set(['N. Russell St.', 'N. University St.']),\n",
      " 'W': set(['Sagamore Pkwy W']),\n",
      " 'Way': set(['Geddes Way']),\n",
      " 'West': set(['Sagamore Parkway West'])}\n"
     ]
    }
   ],
   "source": [
    "street_types = audit(OSMFILE)\n",
    "\n",
    "pprint.pprint(dict(street_types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going through the street name list, I will use it to update the 'mapping' list. In this list I mention the format of the street type that was found in the file (left) and specify to what format it needs to be changed (right).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The list of dictionaries, containing street types that need to be changed to match the expected list\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"street\": \"Street\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Ave.\": \"Avenue\",\n",
    "            \"AVE\": \"Avenue,\",\n",
    "            \"avenue\": \"Avenue\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Rd\": \"Road\",\n",
    "            \"road\": \"Road\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"Blvd.\": \"Boulevard\",\n",
    "            \"Blvd,\": \"Boulevard\",\n",
    "            \"boulevard\": \"Boulevard\",\n",
    "            \"broadway\": \"Broadway\",\n",
    "            \"square\": \"Square\",\n",
    "            \"way\": \"Way\",\n",
    "            \"Dr.\": \"Drive\",\n",
    "            \"Dr\": \"Drive\",\n",
    "            \"ct\": \"Court\",\n",
    "            \"Ct\": \"Court\",\n",
    "            \"court\": \"Court\",\n",
    "            \"Sq\": \"Square\",\n",
    "            \"square\": \"Square\",\n",
    "            \"cres\": \"Crescent\",\n",
    "            \"Cres\": \"Crescent\",\n",
    "            \"Ctr\": \"Center\",\n",
    "            \"Hwy\": \"Highway\",\n",
    "            \"hwy\": \"Highway\",\n",
    "            \"Ln\": \"Lane\",\n",
    "            \"Ln.\": \"Lane\",\n",
    "            \"parkway\": \"Parkway\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To match the expected list of street name and replace the abbreviated street types, I wrote a function that uses the mapping to do this conversion.\n",
    "\n",
    "I take the street name and split it at the space character. In case I could find a string that matches any in the mapping, I replace it with the format I have specified for it. When the function finds 'Blvd', it goes through mapping and map it to 'Boulevard', and the final street name will come out as 'N California Boulevard'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_name(name, mapping):\n",
    "    output = list()\n",
    "    parts = name.split(\" \")\n",
    "    for part in parts:\n",
    "        if part in mapping:\n",
    "            output.append(mapping[part])\n",
    "        else:\n",
    "            output.append(part)\n",
    "    return \" \".join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a print to see how the changes have been applied. I iterate through the street_types from which collected different street types from the 'audit' function, and call the 'update_name' function to change the street type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "West 350 North => West 350 North\n",
      "Sagamore Pkwy W => Sagamore Pkwy W\n",
      "Sagamore Parkway West => Sagamore Parkway West\n",
      "N. Russell St. => N. Russell Street\n",
      "N. University St. => N. University Street\n",
      "US 52 => US 52\n",
      "Klondike Rd => Klondike Road\n",
      "Stadium Mall => Stadium Mall\n",
      "Purdue Mall => Purdue Mall\n",
      "Memorial Mall => Memorial Mall\n",
      "Geddes Way => Geddes Way\n",
      "Hamilton & Lagrange => Hamilton & Lagrange\n",
      "W. Stadium Ave. => W. Stadium Avenue\n",
      "East State St => East State Street\n",
      "W Wood St => W Wood Street\n",
      "Agriculture Mall Dr => Agriculture Mall Drive\n",
      "Cumberland Ave #D => Cumberland Avenue #D\n"
     ]
    }
   ],
   "source": [
    "for st_type, ways in street_types.iteritems():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping)\n",
    "            print name, \"=>\", better_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auditing Postcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postcodes are another inconsistent type of data that is entered into the map. The inconsistency is either in how they are represented (with the city abbreviation or without) or how long they are.\n",
    "\n",
    "In the 'dicti' function, I create a dictionary where I can store postcodes. The dictionary key will be the postcode itself and the dictionary value will be the number of times that postcode was repeated throughout the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OSMFILE = 'map.osm'\n",
    "\n",
    "def dicti(data, item):\n",
    "    data[item] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'get_postcode' function will take the 'tag' element as an input and return the elements for which the keys are equal to 'addr:postcode' \n",
    "\n",
    "The 'audit' function, like the one for street names, parses the XML file and iterates through node and way elements. It extracts the value attribute (i.e. the postcode) and add it to the 'dicti' dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_postcode(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    data = defaultdict(int)\n",
    "    # parsing the XML file\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        \n",
    "        # iterating through node and way elements.\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if get_postcode(tag):\n",
    "                    dicti(data, tag.attrib['v'])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will call the 'audit' function and print the output which should be a list of dictionaries of postcodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'47406': 1, '47906': 57, '47907': 43}\n"
     ]
    }
   ],
   "source": [
    "postcodes = audit(OSMFILE)\n",
    "\n",
    "pprint.pprint(dict(postcodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Postcodes and Ways to Clean Them up\n",
    "The output shows that the postcodes are in these formats:\n",
    "- A 5-digit format (e.g. 12345)\n",
    "- A 5-digit format followed by more numbers after a hyphen (e.g. 12345-6789)\n",
    "\n",
    "To deal with the postcodes, I divide them into different categories:\n",
    "- First category include the ones:\n",
    "    - Where the length equals to 5 (e.g. 12345)\n",
    "    - Where the length is longer than 5, and they contain characters (like abbreviations of a city) (e.g. CA 12345)\n",
    "    \n",
    "- Second category include the ones:\n",
    "    - Where the length is longer than 5, and they are followed by a hyphen (e.g. 12345-6789)\n",
    "    \n",
    "- Third category include the ones:\n",
    "    - Where the length is longer than 5, but are not followed by any hyphen (e.g. 123456)\n",
    "    - Where the length is shorter than 5 (e.g. 1234, 515)\n",
    "    - Where the postcode equals to 'CA'\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_postcode(digit):\n",
    "    output = list()\n",
    "    \n",
    "    first_category = re.compile(r'^\\D*(\\d{5}$)', re.IGNORECASE)\n",
    "    \n",
    "    second_category = re.compile('^(\\d{5})-\\d{4}$')\n",
    "    \n",
    "    third_category = re.compile('^\\d{6}$')\n",
    "    \n",
    "    if re.search(first_category, digit):\n",
    "        new_digit = re.search(first_category, digit).group(1)\n",
    "        output.append(new_digit)\n",
    "        \n",
    "    elif re.search(second_category, digit):\n",
    "        new_digit = re.search(second_category, digit).group(1)\n",
    "        output.append(new_digit)\n",
    "    \n",
    "    elif re.search(third_category, digit):\n",
    "        third_output = third_category.search(digit)\n",
    "        new_digit = '00000'\n",
    "        output.append('00000')\n",
    "    \n",
    "    # this condition matches the third category for the other two types of postcodes\n",
    "    elif digit == 'CA' or len(digit) < 5:\n",
    "        new_digit = '00000'\n",
    "        output.append(new_digit)\n",
    "\n",
    "    return ', '.join(str(x) for x in output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will print the output after the changes are done to the postcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for postcode, nums in postcodes.iteritems():\n",
    "    better_code = update_postcode(postcode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data for the Database\n",
    "\n",
    "To load the data to the SQLite database, I need to transfer it from the XML file to CSV files. I create multiple CSV files, and later create the corresponding tables in my database based on them.\n",
    "\n",
    "The CSV files I want to have are:\n",
    "- Node\n",
    "- Node_tags\n",
    "- Way\n",
    "- Way_tags\n",
    "- Way_nodes\n",
    "\n",
    "Each of these CSV files contains different columns and stores data based on those columns. The columns used in the CSV files will be the table columns in the database. This is the schema:\n",
    "- NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "- NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "- WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "- WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "- WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "def shape_element(element):\n",
    "\n",
    "    node_attribs = {} # Handle the attributes in node element\n",
    "    way_attribs = {} # Handle the attributes in way element\n",
    "    way_nodes = [] # Handle the 'nd' tag in the way element\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "    \n",
    "    # Handling node elements\n",
    "    if element.tag == 'node':\n",
    "        for item in NODE_FIELDS:\n",
    "            try:\n",
    "                node_attribs[item] = element.attrib[item]\n",
    "            except:\n",
    "                node_attribs[item] = \"9999999\"\n",
    "        \n",
    "        # Iterating through the 'tag' tags in the node element\n",
    "        for tg in element.iter('tag'):\n",
    "            if not PROBLEMCHARS.search(tg.attrib['k']):\n",
    "                tag_dict_node = {}\n",
    "                tag_dict_node['id'] = element.attrib['id']\n",
    "\n",
    "                # Calling the update_name function to clean up problematic street names based on audit.py file\n",
    "                if is_street_name(tg):\n",
    "                    better_name = update_name(tg.attrib['v'], mapping)\n",
    "                    tag_dict_node['value'] = better_name\n",
    "\n",
    "                # Calling the update_postcode function to clean up problematic postcodes based on audit.py file\n",
    "                elif get_postcode(tg):\n",
    "                    better_postcode = update_postcode(tg.attrib['v'])\n",
    "                    tag_dict_node['value'] = better_postcode\n",
    "                \n",
    "                # For other values that are not street names or postcodes\n",
    "                else:\n",
    "                    tag_dict_node['value'] = tg.attrib['v']\n",
    "\n",
    "                if ':' not in tg.attrib['k']:\n",
    "                    tag_dict_node['key'] = tg.attrib['k']\n",
    "                    tag_dict_node['type'] = 'regular'\n",
    "                else: \n",
    "                    character_before_colon = re.findall('^[a-zA-Z]*:', tg.attrib['k'])\n",
    "                    character_after_colon = re.findall(':[a-zA-Z_]+' , tg.attrib['k'])\n",
    "                    if len(character_after_colon) != 0:\n",
    "                        tag_dict_node['key'] = character_after_colon[0][1:]\n",
    "                    else:\n",
    "                        tag_dict_node['key'] = 'regular'\n",
    "\n",
    "                    if len(character_before_colon) != 0:\n",
    "                        tag_dict_node['type'] = character_before_colon[0][: -1]\n",
    "                    else:\n",
    "                        tag_dict_node['type'] = 'regular'\n",
    "                tags.append(tag_dict_node)\n",
    "            \n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "        \n",
    "    # Handling way elements\n",
    "    elif element.tag == 'way':\n",
    "        for item in WAY_FIELDS:\n",
    "            try:\n",
    "                way_attribs[item] = element.attrib[item]\n",
    "            except:\n",
    "                way_attribs[item] = \"9999999\"\n",
    "        \n",
    "        # Iterating through 'tag' tags in way element\n",
    "        for tg in element.iter('tag'):\n",
    "            if not PROBLEMCHARS.search(tg.attrib['k']):\n",
    "                tag_dict_way = {}\n",
    "                tag_dict_way['id'] = element.attrib['id']\n",
    "\n",
    "                # Calling the update_name function to clean up problematic street names based on audit.py file\n",
    "                if is_street_name(tg):\n",
    "                    better_name_way = update_name(tg.attrib['v'], mapping)\n",
    "                    tag_dict_way['value'] = better_name_way\n",
    "\n",
    "                # Calling the update_postcode function to clean up problematic postcodes based on audit.py file\n",
    "                if get_postcode(tg):\n",
    "                    better_postcode_way = update_postcode(tg.attrib['v'])\n",
    "                    tag_dict_way['value'] = better_postcode_way\n",
    "\n",
    "                # For other values that are not street names or postcodes\n",
    "                else:\n",
    "                    tag_dict_way['value'] = tg.attrib['v']\n",
    "\n",
    "                if ':' not in tg.attrib['k']:\n",
    "                    tag_dict_way['key'] = tg.attrib['k']\n",
    "                    tag_dict_way['type'] = 'regular'\n",
    "                else:\n",
    "                    character_before_colon = re.findall('^[a-zA-Z]*:', tg.attrib['k'])\n",
    "                    character_after_colon = re.findall(':[a-zA-Z_]+', tg.attrib['k'])\n",
    "                \n",
    "                    if len(character_after_colon) == 1:\n",
    "                        tag_dict_way['key'] = character_after_colon[0][1:]\n",
    "                    if len(character_after_colon) > 1:\n",
    "                        tag_dict_way['key'] = character_after_colon[0][1: ] + character_after_colon[1]\n",
    "                \n",
    "                    if len(character_before_colon) != 0:\n",
    "                        tag_dict_way['type'] = character_before_colon[0][: -1]\n",
    "                    else:\n",
    "                        tag_dict_way['type'] = 'regular'\n",
    "                \n",
    "                tags.append(tag_dict_way)\n",
    "        \n",
    "        # Iterating through 'nd' tags in way element\n",
    "        count = 0\n",
    "        for tg in element.iter('nd'):\n",
    "            tag_dict_nd = {}\n",
    "            tag_dict_nd['id'] = element.attrib['id']\n",
    "            tag_dict_nd['node_id'] = tg.attrib['ref']\n",
    "            tag_dict_nd['position'] = count\n",
    "            count += 1\n",
    "            \n",
    "            way_nodes.append(tag_dict_nd)\n",
    "        \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the shape_element function in place, I can now parse and shape the data, and write it to CSV files.\n",
    "\n",
    "The main function is what I used to call my audit function to update street names and postcodes. The python script shaping_csv.py takes care of creating the CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "1000000\n",
      "1010000\n",
      "1020000\n",
      "1030000\n",
      "1040000\n",
      "1050000\n",
      "1060000\n",
      "1070000\n",
      "1080000\n",
      "1090000\n",
      "1100000\n",
      "1110000\n",
      "1120000\n",
      "1130000\n",
      "1140000\n",
      "1150000\n",
      "1160000\n",
      "1170000\n",
      "1180000\n",
      "1190000\n",
      "1200000\n",
      "1210000\n",
      "1220000\n",
      "1230000\n",
      "1240000\n",
      "1250000\n",
      "1260000\n",
      "1270000\n",
      "1280000\n",
      "1290000\n",
      "1300000\n",
      "1310000\n",
      "1320000\n",
      "1330000\n",
      "1340000\n",
      "1350000\n",
      "1360000\n",
      "1370000\n",
      "1380000\n",
      "1390000\n",
      "1400000\n",
      "1410000\n",
      "1420000\n",
      "1430000\n",
      "1440000\n",
      "1450000\n",
      "1460000\n",
      "1470000\n",
      "1480000\n",
      "1490000\n",
      "1500000\n",
      "1510000\n",
      "1520000\n",
      "1530000\n",
      "1540000\n",
      "1550000\n",
      "1560000\n",
      "1570000\n",
      "1580000\n",
      "1590000\n",
      "1600000\n",
      "1610000\n",
      "1620000\n",
      "1630000\n",
      "1640000\n",
      "1650000\n",
      "1660000\n",
      "1670000\n",
      "1680000\n",
      "1690000\n",
      "1700000\n",
      "1710000\n",
      "1720000\n",
      "1730000\n",
      "1740000\n",
      "1750000\n",
      "1760000\n",
      "1770000\n",
      "1780000\n",
      "1790000\n",
      "1800000\n",
      "1810000\n",
      "1820000\n",
      "1830000\n",
      "1840000\n",
      "1850000\n",
      "1860000\n",
      "1870000\n",
      "1880000\n",
      "1890000\n",
      "1900000\n",
      "1910000\n",
      "1920000\n",
      "1930000\n",
      "1940000\n",
      "1950000\n",
      "1960000\n",
      "1970000\n",
      "1980000\n",
      "1990000\n",
      "2000000\n",
      "2010000\n",
      "2020000\n",
      "2030000\n",
      "2040000\n",
      "2050000\n",
      "2060000\n",
      "2070000\n",
      "2080000\n",
      "2090000\n",
      "2100000\n",
      "2110000\n",
      "2120000\n",
      "2130000\n",
      "2140000\n",
      "2150000\n",
      "2160000\n",
      "2170000\n",
      "2180000\n",
      "2190000\n",
      "2200000\n",
      "2210000\n",
      "2220000\n",
      "2230000\n",
      "2240000\n",
      "2250000\n",
      "2260000\n",
      "2270000\n",
      "2280000\n",
      "2290000\n",
      "2300000\n",
      "2310000\n",
      "2320000\n",
      "2330000\n",
      "2340000\n",
      "2350000\n",
      "2360000\n",
      "2370000\n",
      "2380000\n",
      "2390000\n",
      "2400000\n",
      "2410000\n",
      "2420000\n",
      "2430000\n",
      "2440000\n",
      "2450000\n",
      "2460000\n",
      "2470000\n",
      "2480000\n",
      "2490000\n",
      "2500000\n",
      "2510000\n",
      "2520000\n",
      "2530000\n",
      "2540000\n",
      "2550000\n",
      "2560000\n",
      "2570000\n",
      "2580000\n",
      "2590000\n",
      "2600000\n",
      "2610000\n",
      "2620000\n",
      "2630000\n",
      "2640000\n",
      "2650000\n",
      "2660000\n",
      "2670000\n",
      "2680000\n",
      "2690000\n",
      "2700000\n",
      "2710000\n",
      "2720000\n",
      "2730000\n",
      "2740000\n",
      "2750000\n",
      "2760000\n",
      "2770000\n",
      "2780000\n",
      "2790000\n",
      "2800000\n",
      "2810000\n",
      "2820000\n",
      "2830000\n",
      "2840000\n",
      "2850000\n",
      "2860000\n",
      "2870000\n",
      "2880000\n",
      "2890000\n",
      "2900000\n",
      "2910000\n",
      "2920000\n",
      "2930000\n",
      "2940000\n",
      "2950000\n",
      "2960000\n",
      "2970000\n",
      "2980000\n",
      "2990000\n",
      "3000000\n",
      "3010000\n",
      "3020000\n",
      "3030000\n",
      "3040000\n",
      "3050000\n",
      "3060000\n",
      "3070000\n",
      "3080000\n",
      "3090000\n",
      "3100000\n",
      "3110000\n",
      "3120000\n",
      "3130000\n",
      "3140000\n",
      "3150000\n",
      "3160000\n",
      "3170000\n",
      "3180000\n",
      "3190000\n",
      "3200000\n",
      "3210000\n",
      "3220000\n",
      "3230000\n",
      "3240000\n",
      "3250000\n",
      "3260000\n",
      "3270000\n",
      "3280000\n",
      "3290000\n",
      "3300000\n",
      "3310000\n",
      "3320000\n",
      "3330000\n",
      "3340000\n",
      "3350000\n",
      "3360000\n",
      "3370000\n",
      "3380000\n",
      "3390000\n",
      "3400000\n",
      "3410000\n",
      "3420000\n",
      "3430000\n",
      "3440000\n",
      "3450000\n",
      "3460000\n",
      "3470000\n",
      "3480000\n",
      "3490000\n",
      "3500000\n",
      "3510000\n",
      "3520000\n",
      "3530000\n",
      "3540000\n",
      "3550000\n",
      "3560000\n",
      "3570000\n",
      "3580000\n",
      "3590000\n",
      "3600000\n",
      "3610000\n",
      "3620000\n",
      "3630000\n",
      "3640000\n",
      "3650000\n",
      "3660000\n",
      "3670000\n",
      "3680000\n",
      "3690000\n",
      "3700000\n",
      "3710000\n",
      "3720000\n",
      "3730000\n",
      "3740000\n",
      "3750000\n",
      "3760000\n",
      "3770000\n",
      "3780000\n",
      "3790000\n",
      "3800000\n",
      "3810000\n",
      "3820000\n",
      "3830000\n",
      "3840000\n",
      "3850000\n",
      "3860000\n",
      "3870000\n",
      "3880000\n",
      "3890000\n",
      "3900000\n",
      "3910000\n",
      "3920000\n",
      "3930000\n",
      "3940000\n",
      "3950000\n",
      "3960000\n",
      "3970000\n",
      "3980000\n",
      "3990000\n",
      "4000000\n",
      "4010000\n",
      "4020000\n",
      "4030000\n",
      "4040000\n",
      "4050000\n",
      "4060000\n",
      "4070000\n",
      "4080000\n",
      "4090000\n",
      "4100000\n",
      "4110000\n",
      "4120000\n",
      "4130000\n",
      "4140000\n",
      "4150000\n",
      "4160000\n",
      "4170000\n",
      "4180000\n",
      "4190000\n",
      "4200000\n",
      "4210000\n",
      "4220000\n",
      "4230000\n",
      "4240000\n",
      "4250000\n",
      "4260000\n",
      "4270000\n",
      "4280000\n",
      "4290000\n",
      "4300000\n",
      "4310000\n",
      "4320000\n",
      "4330000\n",
      "4340000\n",
      "4350000\n",
      "4360000\n",
      "4370000\n",
      "4380000\n",
      "4390000\n",
      "4400000\n",
      "4410000\n",
      "4420000\n",
      "4430000\n",
      "4440000\n",
      "4450000\n",
      "4460000\n",
      "4470000\n",
      "4480000\n",
      "4490000\n",
      "4500000\n",
      "4510000\n",
      "4520000\n",
      "4530000\n",
      "4540000\n",
      "4550000\n",
      "4560000\n",
      "4570000\n",
      "4580000\n",
      "4590000\n",
      "4600000\n",
      "4610000\n",
      "4620000\n",
      "4630000\n",
      "4640000\n",
      "4650000\n",
      "4660000\n",
      "4670000\n",
      "4680000\n",
      "4690000\n",
      "4700000\n",
      "4710000\n",
      "4720000\n",
      "4730000\n",
      "4740000\n",
      "4750000\n",
      "4760000\n",
      "4770000\n",
      "4780000\n",
      "4790000\n",
      "4800000\n",
      "4810000\n",
      "4820000\n",
      "4830000\n",
      "4840000\n",
      "4850000\n",
      "4860000\n",
      "4870000\n",
      "4880000\n",
      "4890000\n",
      "4900000\n",
      "4910000\n",
      "4920000\n",
      "4930000\n",
      "4940000\n",
      "4950000\n",
      "4960000\n",
      "4970000\n",
      "4980000\n",
      "4990000\n",
      "5000000\n",
      "5010000\n",
      "5020000\n",
      "5030000\n",
      "5040000\n",
      "5050000\n",
      "5060000\n",
      "5070000\n",
      "5080000\n",
      "5090000\n",
      "5100000\n",
      "5110000\n",
      "5120000\n",
      "5130000\n",
      "5140000\n",
      "5150000\n",
      "5160000\n",
      "5170000\n",
      "5180000\n",
      "5190000\n",
      "5200000\n",
      "5210000\n",
      "5220000\n",
      "5230000\n",
      "5240000\n",
      "5250000\n",
      "5260000\n",
      "5270000\n",
      "5280000\n",
      "5290000\n",
      "5300000\n",
      "5310000\n",
      "5320000\n",
      "5330000\n",
      "5340000\n",
      "5350000\n",
      "5360000\n",
      "5370000\n",
      "5380000\n",
      "5390000\n",
      "5400000\n",
      "5410000\n",
      "5420000\n",
      "5430000\n",
      "5440000\n",
      "5450000\n",
      "5460000\n",
      "5470000\n",
      "5480000\n",
      "5490000\n",
      "5500000\n",
      "5510000\n",
      "5520000\n",
      "5530000\n",
      "5540000\n",
      "5550000\n",
      "5560000\n",
      "5570000\n",
      "5580000\n",
      "5590000\n",
      "5600000\n",
      "5610000\n",
      "5620000\n",
      "5630000\n",
      "5640000\n",
      "5650000\n",
      "5660000\n",
      "5670000\n",
      "5680000\n",
      "5690000\n",
      "5700000\n",
      "5710000\n",
      "5720000\n",
      "5730000\n",
      "5740000\n",
      "5750000\n",
      "5760000\n",
      "5770000\n",
      "5780000\n",
      "5790000\n",
      "5800000\n",
      "5810000\n",
      "5820000\n",
      "5830000\n",
      "5840000\n",
      "5850000\n",
      "5860000\n",
      "5870000\n",
      "5880000\n",
      "5890000\n",
      "5900000\n",
      "5910000\n",
      "5920000\n",
      "5930000\n",
      "5940000\n",
      "5950000\n",
      "5960000\n",
      "5970000\n",
      "5980000\n",
      "5990000\n",
      "6000000\n",
      "6010000\n",
      "6020000\n",
      "6030000\n",
      "6040000\n",
      "6050000\n",
      "6060000\n",
      "6070000\n",
      "6080000\n",
      "6090000\n",
      "6100000\n",
      "6110000\n",
      "6120000\n",
      "6130000\n",
      "6140000\n",
      "6150000\n",
      "6160000\n",
      "6170000\n",
      "6180000\n",
      "6190000\n",
      "6200000\n",
      "6210000\n",
      "6220000\n",
      "6230000\n",
      "6240000\n",
      "6250000\n",
      "6260000\n",
      "6270000\n",
      "6280000\n",
      "6290000\n",
      "6300000\n",
      "6310000\n",
      "6320000\n",
      "6330000\n",
      "6340000\n",
      "6350000\n",
      "6360000\n",
      "6370000\n",
      "6380000\n",
      "6390000\n",
      "6400000\n",
      "6410000\n",
      "6420000\n",
      "6430000\n",
      "6440000\n",
      "6450000\n",
      "6460000\n",
      "6470000\n",
      "6480000\n",
      "6490000\n",
      "6500000\n",
      "6510000\n",
      "6520000\n",
      "6530000\n",
      "6540000\n",
      "6550000\n",
      "6560000\n",
      "6570000\n",
      "6580000\n",
      "6590000\n",
      "6600000\n",
      "6610000\n",
      "6620000\n",
      "6630000\n",
      "6640000\n",
      "6650000\n",
      "6660000\n",
      "6670000\n",
      "6680000\n",
      "6690000\n",
      "6700000\n",
      "6710000\n",
      "6720000\n",
      "6730000\n",
      "6740000\n",
      "6750000\n",
      "6760000\n",
      "6770000\n",
      "6780000\n",
      "6790000\n",
      "6800000\n",
      "6810000\n",
      "6820000\n",
      "6830000\n",
      "6840000\n",
      "6850000\n",
      "6860000\n",
      "6870000\n",
      "6880000\n",
      "6890000\n",
      "6900000\n",
      "6910000\n",
      "6920000\n",
      "6930000\n",
      "6940000\n",
      "6950000\n",
      "6960000\n",
      "6970000\n",
      "6980000\n",
      "6990000\n",
      "7000000\n",
      "7010000\n",
      "7020000\n",
      "7030000\n",
      "7040000\n",
      "7050000\n",
      "7060000\n",
      "7070000\n",
      "7080000\n",
      "7090000\n",
      "7100000\n",
      "7110000\n",
      "7120000\n",
      "7130000\n",
      "7140000\n",
      "7150000\n",
      "7160000\n",
      "7170000\n",
      "7180000\n",
      "7190000\n",
      "7200000\n",
      "7210000\n",
      "7220000\n",
      "7230000\n",
      "7240000\n",
      "7250000\n",
      "7260000\n",
      "7270000\n",
      "7280000\n",
      "7290000\n",
      "7300000\n",
      "7310000\n",
      "7320000\n",
      "7330000\n",
      "7340000\n",
      "7350000\n",
      "7360000\n",
      "7370000\n",
      "7380000\n",
      "7390000\n",
      "7400000\n",
      "7410000\n",
      "7420000\n",
      "7430000\n",
      "7440000\n",
      "7450000\n",
      "7460000\n",
      "7470000\n",
      "7480000\n",
      "7490000\n",
      "7500000\n",
      "7510000\n",
      "7520000\n",
      "7530000\n",
      "7540000\n",
      "7550000\n",
      "7560000\n",
      "7570000\n",
      "7580000\n",
      "7590000\n",
      "7600000\n",
      "7610000\n",
      "7620000\n",
      "7630000\n",
      "7640000\n",
      "7650000\n",
      "7660000\n",
      "7670000\n",
      "7680000\n",
      "7690000\n",
      "7700000\n",
      "7710000\n",
      "7720000\n",
      "7730000\n",
      "7740000\n",
      "7750000\n",
      "7760000\n",
      "7770000\n",
      "7780000\n",
      "7790000\n",
      "7800000\n",
      "7810000\n",
      "7820000\n",
      "7830000\n",
      "7840000\n",
      "7850000\n",
      "7860000\n",
      "7870000\n",
      "7880000\n",
      "7890000\n",
      "7900000\n",
      "7910000\n",
      "7920000\n",
      "7930000\n",
      "7940000\n",
      "7950000\n",
      "7960000\n",
      "7970000\n",
      "7980000\n",
      "7990000\n",
      "8000000\n",
      "8010000\n",
      "8020000\n",
      "8030000\n",
      "8040000\n",
      "8050000\n",
      "8060000\n",
      "8070000\n",
      "8080000\n",
      "8090000\n",
      "8100000\n",
      "8110000\n",
      "8120000\n",
      "8130000\n",
      "8140000\n",
      "8150000\n",
      "8160000\n",
      "8170000\n",
      "8180000\n",
      "8190000\n",
      "8200000\n",
      "8210000\n",
      "8220000\n",
      "8230000\n",
      "8240000\n",
      "8250000\n",
      "8260000\n",
      "8270000\n",
      "8280000\n",
      "8290000\n",
      "8300000\n",
      "8310000\n",
      "8320000\n",
      "8330000\n",
      "8340000\n",
      "8350000\n",
      "8360000\n",
      "8370000\n",
      "8380000\n",
      "8390000\n",
      "8400000\n",
      "8410000\n",
      "8420000\n",
      "8430000\n",
      "8440000\n",
      "8450000\n",
      "8460000\n",
      "8470000\n",
      "8480000\n",
      "8490000\n",
      "8500000\n",
      "8510000\n",
      "8520000\n",
      "8530000\n",
      "8540000\n",
      "8550000\n",
      "8560000\n",
      "8570000\n",
      "8580000\n",
      "8590000\n",
      "8600000\n",
      "8610000\n",
      "8620000\n",
      "8630000\n",
      "8640000\n",
      "8650000\n",
      "8660000\n",
      "8670000\n",
      "8680000\n",
      "8690000\n",
      "8700000\n",
      "8710000\n",
      "8720000\n",
      "8730000\n",
      "8740000\n",
      "8750000\n",
      "8760000\n",
      "8770000\n",
      "8780000\n",
      "8790000\n",
      "8800000\n",
      "8810000\n",
      "8820000\n",
      "8830000\n",
      "8840000\n",
      "8850000\n",
      "8860000\n",
      "8870000\n",
      "8880000\n",
      "8890000\n",
      "8900000\n",
      "8910000\n",
      "8920000\n",
      "8930000\n",
      "8940000\n",
      "8950000\n",
      "8960000\n",
      "8970000\n",
      "8980000\n",
      "8990000\n",
      "9000000\n",
      "9010000\n",
      "9020000\n",
      "9030000\n",
      "9040000\n",
      "9050000\n",
      "9060000\n",
      "9070000\n",
      "9080000\n",
      "9090000\n",
      "9100000\n",
      "9110000\n",
      "9120000\n",
      "9130000\n",
      "9140000\n",
      "9150000\n",
      "9160000\n",
      "9170000\n",
      "9180000\n",
      "9190000\n",
      "9200000\n",
      "9210000\n",
      "9220000\n",
      "9230000\n",
      "9240000\n",
      "9250000\n",
      "9260000\n",
      "9270000\n",
      "9280000\n",
      "9290000\n",
      "9300000\n",
      "9310000\n",
      "9320000\n",
      "9330000\n",
      "9340000\n",
      "9350000\n",
      "9360000\n",
      "9370000\n",
      "9380000\n",
      "9390000\n",
      "9400000\n",
      "9410000\n",
      "9420000\n",
      "9430000\n",
      "9440000\n",
      "9450000\n",
      "9460000\n",
      "9470000\n",
      "9480000\n",
      "9490000\n",
      "9500000\n",
      "9510000\n",
      "9520000\n",
      "9530000\n",
      "9540000\n",
      "9550000\n",
      "9560000\n",
      "9570000\n",
      "9580000\n",
      "9590000\n",
      "9600000\n",
      "9610000\n",
      "9620000\n",
      "9630000\n",
      "9640000\n",
      "9650000\n",
      "9660000\n",
      "9670000\n",
      "9680000\n",
      "9690000\n",
      "9700000\n",
      "9710000\n",
      "9720000\n",
      "9730000\n",
      "9740000\n",
      "9750000\n",
      "9760000\n",
      "9770000\n",
      "9780000\n",
      "9790000\n",
      "9800000\n",
      "9810000\n",
      "9820000\n",
      "9830000\n",
      "9840000\n",
      "9850000\n",
      "9860000\n",
      "9870000\n",
      "9880000\n",
      "9890000\n",
      "9900000\n",
      "9910000\n",
      "9920000\n",
      "9930000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "After auditing is complete the next step is to prepare the data to be inserted into a SQL database.\n",
    "To do so I will parse the elements in the OSM XML file, transforming them from document format to\n",
    "tabular format, thus making it possible to write to .csv files.  These csv files can then easily be\n",
    "imported to a SQL database as tables.\n",
    "\"\"\"\n",
    "\n",
    "#Initial imports\n",
    "import csv\n",
    "import codecs\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "from audit import *  #Imports all the functions from audit.py file\n",
    "import cerberus\n",
    "import schema\n",
    "\n",
    "#The directory where the OSM file is located\n",
    "OSM_PATH = 'chicago_illinois.osm'\n",
    "\n",
    "#The directory where the created CSV files will be located \n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "#The SQL schema that is defined in schema.py file. Both files need to be in the same directory.\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "#Regular expression pattern to find problematic characters in value attributes\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "#Regular expression pattern to find different types of streets in street names\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "#The list of street types that we want to have\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\"]\n",
    "\n",
    "#The list of dictionaries, containing street types that need to be changed to match the 'expected' list\n",
    "mapping = { \"St\": \"Street\", \"St.\": \"Street\", \"street\": \"Street\",\n",
    "            \"Ave\": \"Avenue\", \"Ave.\": \"Avenue\", \"AVE\": \"Avenue,\", \"avenue\": \"Avenue\",\n",
    "            \"Rd.\": \"Road\", \"Rd\": \"Road\", \"road\": \"Road\",\n",
    "            \"Blvd\": \"Boulevard\", \"Blvd.\": \"Boulevard\", \"Blvd,\": \"Boulevard\", \"boulevard\": \"Boulevard\",\n",
    "            \"broadway\": \"Broadway\",\n",
    "            \"square\": \"Square\", \"square\": \"Square\", \"Sq\": \"Square\",\n",
    "            \"way\": \"Way\",\n",
    "            \"Dr.\": \"Drive\", \"Dr\": \"Drive\",\n",
    "            \"ct\": \"Court\", \"Ct\": \"Court\", \"court\": \"Court\",\n",
    "            \"cres\": \"Crescent\", \"Cres\": \"Crescent\", \"Ctr\": \"Center\",\n",
    "            \"Hwy\": \"Highway\", \"hwy\": \"Highway\",\n",
    "            \"Ln\": \"Lane\", \"Ln.\": \"Lane\",\n",
    "            \"parkway\": \"Parkway\" }\n",
    "\n",
    "#The columns in the CSV files. The same columns need to be created for the database\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    " \n",
    "def shape_element(element):\n",
    "    \"\"\" A function to Shape each element into several data structures.\n",
    "\n",
    "    Arg:\n",
    "    -param element: 'Node' and'way' tags that are passed to this function from\n",
    "    the get_element function; mainly called by process_map function\n",
    "\n",
    "    The function goes through node and way elements, defining the values for\n",
    "    nodes, nodes_ways, ways, ways_tags, and ways_nodes dictionaties. \n",
    "    \n",
    "    The \"node\" field holds a dictionary of the following top level node \n",
    "    attributes: id, user, uid, version, lat, lon, timestamp, changeset\n",
    "\n",
    "    The \"ways\" fields hold a dictionary of the following top level node\n",
    "    attributes: id, users, uid, versiob, timestamp, changeset\n",
    "    \n",
    "    The \"node_tags\" and \"way_tags\"field holds a list of dictionaries, one per \n",
    "    secondary tag. Secondary tags are child tags of node which have the tag \n",
    "    name/type: \"tag\". Each dictionary has the following fields from the secondary\n",
    "    tag attributes:\n",
    "    - id: the top level node id attribute value\n",
    "    - key: the full tag \"k\" attribute value if no colon is present or the \n",
    "        characters after the colon if one is.\n",
    "    - value: the tag \"v\" attribute value\n",
    "    - type: either the characters before the colon in the tag \"k\" value \n",
    "        or \"regular\" if a colon is not present.\n",
    "    For the value field, I call updates_name and update_postcode functions to\n",
    "    clean problematic street names or postcodes. I call these functions on both\n",
    "    node and way elements.\n",
    "\n",
    "    Return:\n",
    "    The following dictionaries will be returned:\n",
    "    -node\n",
    "    -node_tags\n",
    "    -way\n",
    "    -way_nodes\n",
    "    -way_tags\n",
    "    \"\"\"\n",
    "    node_attribs = {} # Handle the attributes in node element\n",
    "    way_attribs = {} # Handle the attributes in way element\n",
    "    way_nodes = [] # Handle the 'nd' tag in the way element\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "    \n",
    "    # Handling node elements\n",
    "    if element.tag == 'node':\n",
    "        for item in NODE_FIELDS:\n",
    "            #If the 'uid' field was empty \"9999999\" is set as 'uid'\n",
    "            try:\n",
    "                node_attribs[item] = element.attrib[item]\n",
    "            except:\n",
    "                node_attribs[item] = \"9999999\"\n",
    "        \n",
    "        # Iterating through the 'tag' tags in the node element\n",
    "        for tg in element.iter('tag'):\n",
    "            if not PROBLEMCHARS.search(tg.attrib['k']): #Ignoring values that contain problematic characters\n",
    "                tag_dict_node = {}\n",
    "                tag_dict_node['id'] = element.attrib['id']\n",
    "\n",
    "                # Calling the update_name function to clean up problematic street names based on audit.py file\n",
    "                if is_street_name(tg):\n",
    "                    better_name = update_name(tg.attrib['v'], mapping)\n",
    "                    tag_dict_node['value'] = better_name\n",
    "\n",
    "                # Calling the update_postcode function to clean up problematic postcodes based on audit.py file\n",
    "                elif get_postcode(tg):\n",
    "                    better_postcode = update_postcode(tg.attrib['v'])\n",
    "                    tag_dict_node['value'] = better_postcode\n",
    "                \n",
    "                # For other values that are not street names or postcodes\n",
    "                else:\n",
    "                    tag_dict_node['value'] = tg.attrib['v']\n",
    "\n",
    "                if ':' not in tg.attrib['k']:\n",
    "                    tag_dict_node['key'] = tg.attrib['k']\n",
    "                    tag_dict_node['type'] = 'regular'\n",
    "                #Dividing words before and after a colon ':'\n",
    "                else: \n",
    "                    character_before_colon = re.findall('^[a-zA-Z]*:', tg.attrib['k'])\n",
    "                    character_after_colon = re.findall(':[a-zA-Z_]+' , tg.attrib['k'])\n",
    "                    if len(character_after_colon) != 0: #If the key was an empty field\n",
    "                        tag_dict_node['key'] = character_after_colon[0][1:]\n",
    "                    else:\n",
    "                        tag_dict_node['key'] = 'regular'\n",
    "\n",
    "                    if len(character_before_colon) != 0: #If the type was an empty field\n",
    "                        tag_dict_node['type'] = character_before_colon[0][: -1]\n",
    "                    else:\n",
    "                        tag_dict_node['type'] = 'regular'\n",
    "                tags.append(tag_dict_node)\n",
    "            \n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "        \n",
    "    # Handling way elements\n",
    "    elif element.tag == 'way':\n",
    "        for item in WAY_FIELDS:\n",
    "            #If the 'uid' field was empty \"9999999\" is set as 'uid'\n",
    "            try:\n",
    "                way_attribs[item] = element.attrib[item]\n",
    "            except:\n",
    "                way_attribs[item] = \"9999999\"\n",
    "        \n",
    "        # Iterating through 'tag' tags in way element\n",
    "        for tg in element.iter('tag'):\n",
    "            if not PROBLEMCHARS.search(tg.attrib['k']):\n",
    "                tag_dict_way = {}\n",
    "                tag_dict_way['id'] = element.attrib['id']\n",
    "\n",
    "                # Calling the update_name function to clean up problematic street names based on audit.py file\n",
    "                if is_street_name(tg):\n",
    "                    better_name_way = update_name(tg.attrib['v'], mapping)\n",
    "                    tag_dict_way['value'] = better_name_way\n",
    "\n",
    "                # Calling the update_postcode function to clean up problematic postcodes based on audit.py file\n",
    "                if get_postcode(tg):\n",
    "                    better_postcode_way = update_postcode(tg.attrib['v'])\n",
    "                    tag_dict_way['value'] = better_postcode_way\n",
    "\n",
    "                # For other values that are not street names or postcodes\n",
    "                else:\n",
    "                    tag_dict_way['value'] = tg.attrib['v']\n",
    "\n",
    "                if ':' not in tg.attrib['k']:\n",
    "                    tag_dict_way['key'] = tg.attrib['k']\n",
    "                    tag_dict_way['type'] = 'regular'\n",
    "                #Dividing words before and after a colon ':'\n",
    "                else:\n",
    "                    character_before_colon = re.findall('^[a-zA-Z]*:', tg.attrib['k'])\n",
    "                    character_after_colon = re.findall(':[a-zA-Z_]+', tg.attrib['k'])\n",
    "                \n",
    "                    if len(character_after_colon) == 1:\n",
    "                        tag_dict_way['key'] = character_after_colon[0][1:]\n",
    "                    if len(character_after_colon) > 1:\n",
    "                        tag_dict_way['key'] = character_after_colon[0][1: ] + character_after_colon[1]\n",
    "                \n",
    "                    if len(character_before_colon) != 0: #If the type was an empty field\n",
    "                        tag_dict_way['type'] = character_before_colon[0][: -1]\n",
    "                    else:\n",
    "                        tag_dict_way['type'] = 'regular'\n",
    "                \n",
    "                tags.append(tag_dict_way)\n",
    "        \n",
    "        # Iterating through 'nd' tags in way element\n",
    "        count = 0\n",
    "        for tg in element.iter('nd'):\n",
    "            tag_dict_nd = {}\n",
    "            tag_dict_nd['id'] = element.attrib['id']\n",
    "            tag_dict_nd['node_id'] = tg.attrib['ref']\n",
    "            tag_dict_nd['position'] = count\n",
    "            count += 1\n",
    "            \n",
    "            way_nodes.append(tag_dict_nd)\n",
    "        \n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#   Helper Functions - Written by Udacity Lecturers  #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "#Validating that during creation of CSV files the fields are all in accordance with the columns that should be\n",
    "#in the CSV files\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "# =================================================================== #\n",
    "#  Main Function-Creating the CSV files-Written by Udacity Lecturers  #\n",
    "# =================================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        count = 1\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            if count % 10000 == 0: #Setting a counter to show how many rows the code has processed\n",
    "                print count\n",
    "            count += 1\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: If the validation is set to True, the process takes much longer than when it is set to False\n",
    "\n",
    "    process_map(OSM_PATH, validate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Database from above CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Build database of the CSV files with the repective table names.\n",
    "\"\"\"\n",
    "\n",
    "import csv, sqlite3\n",
    "\n",
    "con = sqlite3.connect(\"'db.sqlite'\")\n",
    "con.text_factory = str\n",
    "cur = con.cursor()\n",
    "\n",
    "# create nodes table\n",
    "cur.execute(\"CREATE TABLE nodes (id, lat, lon, user, uid, version, changeset, timestamp);\")\n",
    "with open('nodes.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'], i['lat'], i['lon'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp']) \\\n",
    "             for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO nodes (id, lat, lon, user, uid, version, changeset, timestamp) \\\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?);\", to_db)\n",
    "con.commit()\n",
    "\n",
    "#create nodes_tags table\n",
    "cur.execute(\"CREATE TABLE nodes_tags (id, key, value, type);\")\n",
    "with open('nodes_tags.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'], i['key'], i['value'], i['type']) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO nodes_tags (id, key, value, type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "con.commit()\n",
    "\n",
    "#Create ways table\n",
    "cur.execute(\"CREATE TABLE ways (id, user, uid, version, changeset, timestamp);\")\n",
    "with open('ways.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp']) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways (id, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?);\", to_db)\n",
    "con.commit()\n",
    "\n",
    "#Create ways_nodes table\n",
    "cur.execute(\"CREATE TABLE ways_nodes (id, node_id, position);\")\n",
    "with open('ways_nodes.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'], i['node_id'], i['position']) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways_nodes (id, node_id, position) VALUES (?, ?, ?);\", to_db)\n",
    "con.commit()\n",
    "\n",
    "#Create ways_tags table\n",
    "cur.execute(\"CREATE TABLE ways_tags (id, key, value, type);\")\n",
    "with open('ways_tags.csv','rb') as fin:\n",
    "    dr = csv.DictReader(fin) \n",
    "    to_db = [(i['id'], i['key'], i['value'], i['type']) for i in dr]\n",
    "\n",
    "cur.executemany(\"INSERT INTO ways_tags (id, key, value, type) VALUES (?, ?, ?, ?);\", to_db)\n",
    "con.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to get some information regarding the CSV files and the database I created.\n",
    "\n",
    "By importing 'hurry.filesize' I can translate the file sizes from bytes to KB or MB. To install the library, you need to 'pip install hurry.filesize' it on your machine. I got the idea from using this method from the post below:  \n",
    "https://discussions.udacity.com/t/display-files-and-their-sizes-in-directory/186741"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import os\n",
    "from hurry.filesize import size \n",
    "dirpath = 'submission' #main directory\n",
    "\n",
    "\n",
    "files_list = []\n",
    "for path, dirs, files in os.walk(dirpath):\n",
    "    files_list.extend([(filename, size(os.path.getsize(os.path.join(path, filename)))) for filename in files])\n",
    "\n",
    "for filename, size in files_list:\n",
    "    print '{:.<40s}: {:5s}'.format(filename,size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have audited and cleaned the data and transfered everything into table in my database, I can start running queries on it. The queries answer many questions such as:   \n",
    "- Number of nodes\n",
    "- Number of way\n",
    "- Number of unique users\n",
    "- Most contributing users\n",
    "- Number of users who contributed only once\n",
    "- Top 10 amenitie\n",
    "- Shops \n",
    "- Users who added amenities \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "sqlite_file = 'db.sqlite'\n",
    "con = sqlite3.connect(sqlite_file)\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: \n",
      "8701756\n"
     ]
    }
   ],
   "source": [
    "def number_of_nodes():\n",
    "    output = cur.execute('SELECT COUNT(*) FROM nodes')\n",
    "    return output.fetchone()[0]\n",
    "\n",
    "print 'Number of nodes: \\n' , number_of_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ways: \n",
      "1231106\n"
     ]
    }
   ],
   "source": [
    "def number_of_ways():\n",
    "    output = cur.execute('SELECT COUNT(*) FROM ways')\n",
    "    return output.fetchone()[0]\n",
    "\n",
    "print 'Number of ways: \\n' , number_of_ways()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: \n",
      "2823\n"
     ]
    }
   ],
   "source": [
    "def number_of_unique_users():\n",
    "    output = cur.execute('SELECT COUNT(DISTINCT e.uid) FROM \\\n",
    "                         (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways) e')\n",
    "    return output.fetchone()[0]\n",
    "\n",
    "print 'Number of unique users: \\n' , number_of_unique_users()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most contributing users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most contributing users: \n",
      "\n",
      "[(u'chicago-buildings', 5605968),\n",
      " (u'Umbugbene', 1091115),\n",
      " (u'woodpeck_fixbot', 219369),\n",
      " (u'alexrudd (NHD)', 204341),\n",
      " (u'g246020', 107386),\n",
      " (u'patester24', 105214),\n",
      " (u'mpinnau', 103495),\n",
      " (u'asdf1234', 101397),\n",
      " (u'Oak_Park_IL', 101251),\n",
      " (u'TIGERcnl', 93141)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most_contributing_users():\n",
    "    \n",
    "    output = cur.execute('SELECT e.user, COUNT(*) as num FROM \\\n",
    "                         (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e \\\n",
    "                         GROUP BY e.user \\\n",
    "                         ORDER BY num DESC \\\n",
    "                         LIMIT 10 ')\n",
    "    pprint(output.fetchall())\n",
    "    return output.fetchall()\n",
    "\n",
    "print 'Most contributing users: \\n'\n",
    "most_contributing_users()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of users who contributed once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users who have contributed once: \n",
      "636\n"
     ]
    }
   ],
   "source": [
    "def number_of_users_contributed_once():\n",
    "    \n",
    "    output = cur.execute('SELECT COUNT(*) FROM \\\n",
    "                             (SELECT e.user, COUNT(*) as num FROM \\\n",
    "                                 (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e \\\n",
    "                                  GROUP BY e.user \\\n",
    "                                  HAVING num = 1) u')\n",
    "    \n",
    "    return output.fetchone()[0]\n",
    "                         \n",
    "print 'Number of users who have contributed once: \\n', number_of_users_contributed_once()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 amenities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ten amenities: \n",
      "\n",
      "[(u'place_of_worship', 3038),\n",
      " (u'school', 1906),\n",
      " (u'restaurant', 1568),\n",
      " (u'fast_food', 899),\n",
      " (u'parking', 603),\n",
      " (u'cafe', 450),\n",
      " (u'bench', 426),\n",
      " (u'bicycle_parking', 414),\n",
      " (u'fuel', 388),\n",
      " (u'bicycle_rental', 361),\n",
      " (u'bank', 330),\n",
      " (u'drinking_water', 282),\n",
      " (u'bar', 257),\n",
      " (u'fountain', 229),\n",
      " (u'grave_yard', 216),\n",
      " (u'shelter', 204),\n",
      " (u'fire_station', 192),\n",
      " (u'toilets', 179),\n",
      " (u'pharmacy', 166),\n",
      " (u'pub', 162)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_ten_amenities_in_sf():\n",
    "    output = cur.execute('SELECT value, COUNT(*) as num FROM nodes_tags\\\n",
    "                            WHERE key=\"amenity\" \\\n",
    "                            GROUP BY value \\\n",
    "                            ORDER BY num DESC \\\n",
    "                            LIMIT 20' )\n",
    "    pprint(output.fetchall())\n",
    "    return output.fetchall()\n",
    "\n",
    "print 'Top ten amenities: \\n'\n",
    "top_ten_amenities_in_sf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 cuisines: \n",
      "\n",
      "[(u'burger', 340),\n",
      " (u'mexican', 72),\n",
      " (u'chicken', 64),\n",
      " (u'pizza', 58),\n",
      " (u'american', 53),\n",
      " (u'coffee_shop', 36),\n",
      " (u'sandwich', 31),\n",
      " (u'italian', 30),\n",
      " (u'chinese', 17),\n",
      " (u'ice_cream', 16)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cuisines_in_sf():\n",
    "    output = cur.execute ('SELECT value, COUNT(*) as num FROM ways_tags \\\n",
    "                           WHERE key=\"cuisine\" \\\n",
    "                           GROUP BY value \\\n",
    "                           ORDER BY num DESC \\\n",
    "                           LIMIT 10')\n",
    "    pprint(output.fetchall())\n",
    "    return output.fetchall()\n",
    "\n",
    "print 'Top 10 cuisines: \\n'\n",
    "cuisines_in_sf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different types of shops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users who added amenities to the map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different types of shops: \n",
      "\n",
      "[(u'place_of_worship', 3038),\n",
      " (u'school', 1906),\n",
      " (u'restaurant', 1568),\n",
      " (u'fast_food', 899),\n",
      " (u'parking', 603),\n",
      " (u'cafe', 450),\n",
      " (u'bench', 426),\n",
      " (u'bicycle_parking', 414),\n",
      " (u'fuel', 388),\n",
      " (u'bicycle_rental', 361),\n",
      " (u'bank', 330),\n",
      " (u'drinking_water', 282),\n",
      " (u'bar', 257),\n",
      " (u'fountain', 229),\n",
      " (u'grave_yard', 216),\n",
      " (u'shelter', 204),\n",
      " (u'fire_station', 192),\n",
      " (u'toilets', 179),\n",
      " (u'pharmacy', 166),\n",
      " (u'pub', 162)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def shops_in_sf():\n",
    "    output = cur.execute('SELECT value, COUNT(*) as num FROM nodes_tags\\\n",
    "                            WHERE key=\"shop\" \\\n",
    "                            GROUP BY value \\\n",
    "                            ORDER BY num DESC' )\n",
    "    pprint.pprint(output.fetchall())\n",
    "    return output.fetchall()\n",
    "\n",
    "print 'Different types of shops: \\n'\n",
    "top_ten_amenities_in_sf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users who added amenity to the map: \n",
      "\n",
      "[(u'DACGroup', u'Family Restaurant'),\n",
      " (u'DACGroup', u'Furniture Store'),\n",
      " (u'Dark Asteroid', u'Lombard Village Hall'),\n",
      " (u'DACGroup',\n",
      "  u'Portable Toilet Supplier;Trailer Rental Service;Construction Equipment Supplier;Fence Contractor'),\n",
      " (u'FrankRKryzak', u'arts_centre'),\n",
      " (u'Zol87', u'artwork'),\n",
      " (u'Tomasz11', u'atm'),\n",
      " (u'PhQ', u'baby_hatch'),\n",
      " (u'Tomasz11', u'bank'),\n",
      " (u'Umbugbene', u'banquet_hall')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def users_who_added_amenity():\n",
    "    output = cur.execute('SELECT DISTINCT(nodes.user), nodes_tags.value FROM \\\n",
    "                            nodes join nodes_tags \\\n",
    "                            on nodes.id=nodes_tags.id \\\n",
    "                            WHERE key=\"amenity\" \\\n",
    "                            GROUP BY value \\\n",
    "                            LIMIT 10' ) # Remove this part to view the whole list of users\n",
    "    pprint(output.fetchall())\n",
    "    return output.fetchall()\n",
    "\n",
    "print 'Users who added amenity to the map: \\n'\n",
    "users_who_added_amenity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of postcodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of postcodes: \n",
      "\n",
      "[(u'60201', 9392),\n",
      " (u'60202', 7727),\n",
      " (u'60305', 1720),\n",
      " (u'60564', 1684),\n",
      " (u'60136', 1306)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_of_postcodes():\n",
    "    output = cur.execute('SELECT e.value, COUNT(*) as num FROM \\\n",
    "                            (SELECT value FROM nodes_tags WHERE key=\"postcode\"\\\n",
    "                             UNION ALL SELECT value FROM ways_tags WHERE key=\"postcode\") e \\\n",
    "                            GROUP BY e.value \\\n",
    "                            ORDER BY num DESC \\\n",
    "                            LIMIT 5' ) # Remove this limit to see the complete list of postcodes\n",
    "    pprint(output.fetchall())\n",
    "    return output.fetchall()\n",
    "\n",
    "print 'List of postcodes: \\n'\n",
    "list_of_postcodes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amenities \n",
    "\n",
    "we checked to see what the amenities around this area are. Since the list was quite long, I limited it to the first 20 amenities with the highest number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amenities around  \n",
      "\n",
      "[(u'place_of_worship', 3038),\n",
      " (u'school', 1906),\n",
      " (u'restaurant', 1568),\n",
      " (u'fast_food', 899),\n",
      " (u'parking', 603),\n",
      " (u'cafe', 450),\n",
      " (u'bench', 426),\n",
      " (u'bicycle_parking', 414),\n",
      " (u'fuel', 388),\n",
      " (u'bicycle_rental', 361),\n",
      " (u'bank', 330),\n",
      " (u'drinking_water', 282),\n",
      " (u'bar', 257),\n",
      " (u'fountain', 229),\n",
      " (u'grave_yard', 216),\n",
      " (u'shelter', 204),\n",
      " (u'fire_station', 192),\n",
      " (u'toilets', 179),\n",
      " (u'pharmacy', 166),\n",
      " (u'pub', 162)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def amenities_around_47906():\n",
    "    output = cur.execute('SELECT nodes_tags.value, COUNT(*) as num \\\n",
    "                          FROM nodes_tags \\\n",
    "                            JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE key=\"amenity\") AS amenities \\\n",
    "                            ON nodes_tags.id = amenities.id \\\n",
    "                            WHERE nodes_tags.key=\"amenity\"\\\n",
    "                            GROUP BY nodes_tags.value \\\n",
    "                            ORDER BY num DESC \\\n",
    "                            LIMIT 20' ) # Remove this limit to see the complete list of postcodes\n",
    "    pprint(output.fetchall())\n",
    "    return output.fetchall()\n",
    "\n",
    "print 'Amenities around  \\n'\n",
    "amenities_around_47906()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popular Cafes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most popular cafes: \n",
      "\n",
      "[(u'Starbucks', 50),\n",
      " (u\"Dunkin' Donuts\", 13),\n",
      " (u'Starbucks Coffee', 10),\n",
      " (u\"Peet's Coffee & Tea\", 3),\n",
      " (u'Intelligentsia', 2),\n",
      " (u'Intelligentsia Coffee', 2),\n",
      " (u'Blue Max Coffee', 1),\n",
      " (u'Bow Truss Coffee Roasters', 1),\n",
      " (u'Brew Brew Coffee Lounge', 1),\n",
      " (u'Bridgeport Coffeehouse', 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most_popular_cafes():\n",
    "    output = cur.execute('SELECT nodes_tags.value, COUNT(*) as num \\\n",
    "                          FROM nodes_tags \\\n",
    "                            JOIN (SELECT DISTINCT(id) FROM nodes_tags WHERE value=\"coffee_shop\") AS cafes \\\n",
    "                            ON nodes_tags.id = cafes.id \\\n",
    "                            WHERE nodes_tags.key=\"name\"\\\n",
    "                            GROUP BY nodes_tags.value \\\n",
    "                            ORDER BY num DESC \\\n",
    "                            LIMIT 10' ) # Remove this limit to see the complete list of postcodes\n",
    "    pprint(output.fetchall())\n",
    "    return output.fetchall()\n",
    "\n",
    "print 'Most popular cafes: \\n'\n",
    "most_popular_cafes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "https://github.com/Dalaska/Udacity-Data-Wrangling-Clean-OpenStreetMap\n",
    "\n",
    "https://github.com/bestkao/data-wrangling-with-openstreetmap-and-mongodb\n",
    "\n",
    "https://discussions.udacity.com/t/display-files-and-their-sizes-in-directory/186741\n",
    "\n",
    "https://github.com/Nazaniiin/DataWrangling_OpenStreetMap\n",
    "\n",
    "https://github.com/alphagammamle/Udacity-Data-Analyst-Nanodegree/tree/master/P3-OpenStreetMap-Wrangling-with-SQL\n",
    "\n",
    "https://github.com/alphagammamle/OpenStreetMap-Toronto\n",
    "\n",
    "https://github.com/paul-reiners/udacity-data-wrangling-mongo-db/blob/master/docs/ProjectReport.md\n",
    "\n",
    "http://napitupulu-jon.appspot.com/posts/wrangling-openstreetmap.html\n",
    "\n",
    "http://puwenning.github.io/2016/02/10/P3-project-openstreetmap-data-case-study/\n",
    "\n",
    "http://fch808.github.io/Data%20Wrangling%20with%20MongoDB%20-%20Write-up.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
